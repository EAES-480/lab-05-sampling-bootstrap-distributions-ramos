---
title: "Lab 05 — Sampling and Bootstrap Distributions"
author: "EAES 480 — Modern Statistics in Earth & Environmental Science"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    theme: cosmo
    highlight: tango
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: hide
    df_print: paged
editor_options:
  chunk_output_type: inline
---

```{r setup, echo=FALSE, message=FALSE, warning=T}
library(tidyverse)
library(lubridate)
library(janitor)

knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE
)
```

# Overview

In EAES we almost never observe an entire population. Instead, we take **samples** and compute **point estimates** (like a mean) to infer **population parameters**.

This lab focuses on four core ideas:

1. **Relative error** of point estimates (accuracy)
2. **Standard error (SE)** and how it changes with sample size
3. How **replication** (many repeated samples) creates an approximate **sampling distribution**
4. (Intro) **Bootstrap** distributions: resampling from one sample to approximate uncertainty

## Coding expectations (Week 7+)

This lab uses a mix of:
- **Fill-in-the-blank** (quick checks)
- **Partial pipelines** (you finish the analysis)
- **From-scratch chunks** (you write the full solution)

Use slides + past labs + Discord as needed, but make sure your final code is **your own** and runs **top-to-bottom**.

---

# Data

## Load data (partial pipeline)

Complete the pipeline so `df` is a clean tibble. Filter for 2023, and replace fill values (-9999) with na.

```{r load_data, echo=TRUE, eval=T}
# GOAL: Read the CSV from the repo and clean column names.
# TODO: finish this pipeline.

df <- read_csv("data/us-ams-simple.csv", na = c("-9999")) %>%
  clean_names() %>%
  filter(year_local == 2023)

glimpse(df)
```

## Derive time columns 

Write code to add:
- `date` from `year_local` + `doy`
- `month` as labeled abbreviated month (Jan–Dec)
- `day_night` as "Day"/"Night" from `daytime` (0/1)

```{r derive_time, echo=TRUE, eval=T}
# GOAL: Create date/month/day_night columns.
# HINT: DOY is 1-indexed, so use (doy - 1) with origin "YYYY-01-01".

df <- df %>%
 mutate(
    # TODO: create a Date column from year_local and doy
    # HINT: Jan 1 is DOY = 1, so use (doy - 1) with origin = "YYYY-01-01"
    date = as.Date(doy - 1, origin = paste0(year_local, "-01-01")),

    # TODO: create a month column (numeric 1–12 or labeled months)
    month = month(date, label = TRUE, abbr = TRUE),

    # TODO: make a day/night label using daytime (0/1)
    day_night = if_else(daytime == 1, "Day", "Night")
  )

df %>% count(month)
df %>% count(day_night)
```

---

# Population parameters vs point estimates

## 1) Choose variables (fill-in)

Pick **2–3 variables** to analyze. Include at least **one flux** variable (e.g., `nee`, `fc`, `gpp`) and optionally one meteorological variable (e.g., `ta`, `le`).

```{r choose_vars, echo=TRUE, eval=T}
vars <- c(
  "fc",  # e.g., "fc"
  "gpp",  # e.g., "ta"
  "le"   # optional third variable
)

# CHECK: confirm these columns exist (should return character(0))
setdiff(vars, names(df))
```

---

## 2) Compute population parameters (partial pipeline)

Complete the code to compute **population mean** and **population SD** for each selected variable.

Your output should be a tibble named `pop` with columns: `var`, `mean`, `sd`.

```{r pop_params, echo=TRUE, eval=T}
# GOAL: Create a tidy table of population parameters for vars.
# TODO: complete this pipeline.

pop <- df %>%
  summarise(across(
    all_of(vars),
    list(
      mean = ~mean(.x, na.rm = TRUE),
      sd   = ~sd(.x, na.rm = TRUE)
    ),
    .names = "{.col}__{.fn}"
  )) %>%   
  pivot_longer(
    everything(),
    names_to = c("var", "stat"),
    names_sep = "__",
    values_to = "value"
  ) %>%   
  pivot_wider(names_from = stat, values_from = value)       # TODO: pivot_wider

pop
```

**Prompt (2–3 sentences):** Which variable has the largest population SD? What does that imply about the variability of that process?

> *Write your answer here.*

---

# Relative error for point estimates (accuracy)

We will estimate the population mean with a sample mean, then compute **relative error**:

\[
\text{Relative error (\%)} = 100 \times \frac{|\mu - \bar{x}|}{|\mu|}
\]

## 3) One sample: compute relative error 

Write code to:

1. set a seed
2. choose a sample size `n_samp`
3. draw a simple random sample `samp`
4. compute `sample_mean` for each variable in `vars`
5. join to population means from `pop`
6. compute `rel_error_pct` for each variable

Return a tibble with columns: `var`, `sample_mean`, `pop_mean`, `rel_error_pct`.

```{r one_sample_relerr, echo=TRUE, eval=F}
# GOAL: Relative error for point estimates from one random sample.

set.seed(12)
n_samp <- 100

samp <- df %>%
  slice_sample(n = n_samp)

# get mean point estimate for each variable
est <- samp %>%
  summarise(across(all_of(vars), ~mean(.x, na.rm = T))) %>%
  pivot_longer(everything(), names_to = "var", values_to = "sample_mean")

# compute the relative error for each variable
relerr <- est %>%
  left_join(pop %>% select(var, pop_mean = mean), by = "var") %>%
  mutate(rel_error_pct = 100 * abs(pop_mean - sample_mean) / abs(pop_mean)) %>%
  arrange(desc(rel_error_pct))

relerr
```

**Prompt (3–4 sentences):** Which variable had the largest relative error at your chosen `n_samp`?  
Give a scientific reason why some variables are harder to estimate from a small sample.

> *Write your answer here.*

---

## 4) Relative error vs sample size (partial code + you complete)

We will focus on **one variable** and simulate many repeated studies. The next code chunk is doing quite a lot. Please review the #comments throughout and check you understand generally what each section is doing.

```{r relerr_by_n, echo=TRUE, eval=T}
set.seed(480)

n_grid <- c(25, 50, 100, 200)   # e.g., c(25, 100, 300, 1000)
reps <- 1000                        # e.g., 500 or 1000

target_var <- "fc"                # e.g., "fch4" or "fc"

# Pull the population vector (remove missing values once, up front)
x_pop <- df[[target_var]]
x_pop <- x_pop[!is.na(x_pop)]

# Population mean for the chosen variable
pop_mean_target <- pop %>%
  filter(var == target_var) %>%
  pull(mean)

# ---- Simulation using explicit for loops ----
sim_list <- vector("list", length(n_grid))

for (i in seq_along(n_grid)) {

  n_samp <- n_grid[i]

  # store relative error from each repeated "study"
  relerrs <- numeric(reps)

  for (j in seq_len(reps)) {

    x_samp <- sample(x_pop, size = n_samp, replace = FALSE)
    xbar <- mean(x_samp)

    relerrs[j] <- 100 * abs(pop_mean_target - xbar) / abs(pop_mean_target)
  }

  sim_list[[i]] <- tibble(
    n = n_samp,
    rel_error_pct = relerrs
  )
}

sim_relerr <- bind_rows(sim_list)

sim_relerr
```

### Summarize + plot (from scratch)

1) Summarize the simulated relative error by `n` using:
- median
- 90th percentile

2) Make a plot that communicates “relative error decreases with n”.

```{r summarize_plot_relerr, echo=TRUE, eval=T}
# GOAL: Summarize and plot the relationship between n and relative error.
# TODO: write your own code.

summary_relerr <- sim_relerr %>%
  group_by(n) %>%
  summarize(
    median_relerr = median(rel_error_pct, na.rm = TRUE),
    p90_relerr = quantile(rel_error_pct, 0.90, na.rm = TRUE)
  )
summary_relerr

p_relerr <- ggplot(sim_relerr, aes(x = factor(n), y = rel_error_pct)) +
  geom_boxplot(fill = "lightgreen", alpha = 0.5, outlier.alpha = 0.1) +
  labs(
    title = "Relative error decreases with n",
    x = "n",
    y = "Relative Error (%)"
  ) +
  theme_classic()
p_relerr
```

**Prompt (3–4 sentences):** Describe the pattern. Is improvement from n=25→100 larger than 300→1000?  
Why does accuracy show “diminishing returns” as n increases?

> *Write your answer here.*

---

# Standard error and sampling distributions

For the sample mean, the standard error is approximately:

\[
SE(\bar{x}) \approx \frac{\sigma}{\sqrt{n}}
\]

where \(\sigma\) is population SD.

## 5) Theory vs simulation (partial pipeline)

Compute:
- theoretical SE
- simulated SD of sample means (over many replications)

```{r se_compare, echo=TRUE, eval=T}
set.seed(480)

# TODO: pick a variable and sample size
target_var <- "fc"
n_samp <- 1000
reps <- 50

pop_sd_target <- pop %>% filter(var == target_var) %>% pull(sd)
pop_mean_target <- pop %>% filter(var == target_var) %>% pull(mean)

means <- replicate(
  reps,
  mean(sample(df[[target_var]], size = n_samp, replace = FALSE), na.rm = TRUE)
)

se_theory <- sd(df[[target_var]])/sqrt(length(df[[target_var]]))
se_sim    <- pop_sd_target/sqrt(n_samp)

tibble(
  target_var = target_var,
  n = n_samp,
  se_theory = se_theory,
  se_simulated = se_sim
)
```

### Plot the sampling distribution (fill-in)

```{r plot_sampling_dist, echo=TRUE, eval=T}
tibble(mean_est = means) %>%
  ggplot(aes(x = mean_est)) +
  geom_histogram(bins = 10, alpha = 0.85) +
  geom_vline(xintercept = pop_mean_target, linetype = "dashed", linewidth = 1.1) +
  theme_classic(base_size = 18) +
  labs(
    x = paste0("Sample mean of ", target_var),
    y = "Count",
    title = "Sampling distribution of the mean",
    subtitle = paste0("n = ", n_samp, "; reps = ", reps, " (dashed = population mean)")
  )
```

**Prompt (3–5 sentences):** How close were `se_theory` and `se_simulated`?  
What changes if you increase `n_samp` vs increase `reps`?

> *Write your answer here.*

---

# Bootstrap intuition (one sample, many resamples)

Bootstrap resampling approximates uncertainty **when you only have one sample**.

Key idea: treat your sample as a stand-in for the population and resample **with replacement**.

## 6) Bootstrap distribution of the mean 

Write code to:

- take ONE sample of size `n_samp` from `df`
- generate `B` bootstrap resamples (with replacement) from that sample
- compute bootstrap means
- compute bootstrap SD
- plot the bootstrap distribution

```{r bootstrap_mean, echo=TRUE, eval=T}
set.seed(480)

# TODO: choose settings
target_var <- "fc"
n_samp <- 1000
B <- 200

# one "field campaign" sample
samp <- tibble(x = sample(x_pop, size = n_samp, replace = FALSE))

# bootstrap sampling of the mean
boot_means <- replicate(
  B,
  mean(sample(samp$x, size = n_samp, replace = TRUE))
)

boot_sd <- sd(boot_means)

tibble(
  sample_mean = mean(samp$x),
  boot_sd = boot_sd
)
```

```{r plot_boot, echo=TRUE, eval=T}
# TODO: plot boot_means and mark the sample mean
tibble(boot_means) %>%  
  ggplot(aes(boot_means)) +
  geom_histogram(bins = 10, alpha = 0.85) +
  geom_vline(xintercept = mean(boot_means), linetype = "dashed", linewidth = 1.1) +
  theme_classic(base_size = 18) +
  labs(
    x = paste0("Bootstrap mean of ", target_var),
    y = "Count",
    title = "Bootstrap distribution of the mean",
    subtitle = paste0("n = ", n_samp, "; B = ", B, " (dashed = population mean)")
  )
```

**Prompt (4–6 sentences):** Compare the bootstrap distribution to the sampling distribution from Part III.  
What is the bootstrap trying to approximate? When is bootstrap especially useful in EAES?

> *Write your answer here.*

---

# Part II — Bootstrap under random vs non-random sampling (SWC)

In Part I, you learned that the bootstrap approximates uncertainty **conditional on the sample you observed**.

Now you will test a key warning:

> Bootstrap can quantify *precision* even when the estimate is *biased*.

We will focus on **soil water content (`swc`)**.

- **Population**: all 2023 half-hour records (after removing `NA`)
- **Sample**: one “field campaign” sample of size `n = 200`
- **Bootstrap**: resample *within the sample* (with replacement) many times

## Preparation (recommended)

```{r part2_prep, echo=TRUE, eval=T}
# GOAL: Ensure df exists (loaded in Part I) and create a clean SWC population vector.

swc_pop <- df$swc
swc_pop <- swc_pop[!is.na(swc_pop)]

pop_mean_swc <- mean(swc_pop)
pop_sd_swc   <- sd(swc_pop)

tibble(
  pop_mean_swc = pop_mean_swc,
  pop_sd_swc   = pop_sd_swc,
  n_pop = length(swc_pop)
)
```

---

## Exercise 1 — Random sample → bootstrap distribution (baseline)

**Task:** Draw a *simple random sample* of `n = 200` from the SWC population, then bootstrap the mean.

```{r ex1_random_bootstrap, echo=TRUE, eval=T}
set.seed(480)

n_samp <- 200
B <- 4000

# 1) Draw a random sample (no replacement) from the population vector
swc_samp_random <- sample(swc_pop, size = n_samp, replace = FALSE)

# 2) Bootstrap the mean (resample WITH replacement from the sample)
boot_means_random <- replicate(
  B,
  mean(sample(swc_samp_random, size = n_samp, replace = TRUE))
)

# 3) Summarize
tibble(
  pop_mean = pop_mean_swc,
  sample_mean = mean(swc_samp_random),
  boot_sd = sd(boot_means_random)
)
```

```{r ex1_plot_random_boot, echo=TRUE, eval=T}
# GOAL: Plot the bootstrap distribution and mark (i) sample mean and (ii) population mean.

tibble(boot_mean = boot_means_random) %>%
  ggplot(aes(x = boot_mean)) +
  geom_histogram(bins = 20, alpha = 0.85) +
  geom_vline(xintercept = mean(swc_samp_random), linetype = "dashed", linewidth = 1.1) +
  geom_vline(xintercept = pop_mean_swc, linetype = "solid", linewidth = 1.1) +
  theme_classic(base_size = 18) +
  labs(
    x = "Bootstrap mean (swc)",
    y = "Count",
    title = "Bootstrap distribution (random sample)",
    subtitle = "Dashed = sample mean; Solid = population mean"
  )
```

**Prompt (3–4 sentences):** Is the bootstrap distribution centered near the population mean?  
What does the bootstrap SD represent here?

> *Write your answer here.*

---

## Exercise 2 — Non-random sample (summer-only) → bootstrap distribution (bias example)

**Task:** Create a **summer-only** sample (Jun–Aug) of size `n = 200` and bootstrap the mean.

> This simulates a field campaign that only measured during summer.

```{r ex2_summer_sample, echo=TRUE, eval=T}
# GOAL: Build a summer-only pool of SWC observations (Jun–Aug), then sample from that pool.

# TODO: ensure df has a month column.
# If you do not already have `month`, create it from a date column as in Part I.
# HINT: month(date, label = TRUE, abbr = TRUE)

df_summer <- df %>%
  filter(month %in% c("Jun", "Jul", "Aug")) %>%
  filter(!is.na(swc))

swc_summer_pool <- df_summer$swc

length(swc_summer_pool)
```

```{r ex2_summer_bootstrap, echo=TRUE, eval=T}
set.seed(480)

n_samp <- 200
B <- 4000

# 1) Draw a summer-only sample
swc_samp_summer <- sample(swc_summer_pool, size = n_samp, replace = FALSE)

# 2) Bootstrap the mean from that sample
boot_means_summer <- replicate(
  B,
  mean(sample(swc_samp_summer, size = n_samp, replace = TRUE))
)

# 3) Summarize
tibble(
  pop_mean = pop_mean_swc,
  summer_pool_mean = mean(swc_summer_pool),
  sample_mean = mean(swc_samp_summer),
  boot_sd = sd(boot_means_summer)
)
```

```{r ex2_plot_summer_boot, echo=TRUE, eval=T}
# GOAL: Plot summer bootstrap and compare to population mean.

tibble(boot_mean = boot_means_summer) %>%
  ggplot(aes(x = boot_mean)) +
  geom_histogram(bins = 50, alpha = 0.85) +
  geom_vline(xintercept = mean(swc_samp_summer), linetype = "dashed", linewidth = 1.1) +
  geom_vline(xintercept = pop_mean_swc, linetype = "solid", linewidth = 1.1) +
  theme_classic(base_size = 18) +
  labs(
    x = "Bootstrap mean (swc)",
    y = "Count",
    title = "Bootstrap distribution (summer-only sample)",
    subtitle = "Dashed = sample mean; Solid = population mean"
  )
```

**Prompt (4–6 sentences):** Does the bootstrap distribution look “confident” (narrow) even if it’s centered far from the population mean?  
Explain the difference between **precision** and **accuracy** in this example.  
What *kind* of error is summer-only sampling introducing?

> *Write your answer here.*

---

## Exercise 3 — Stratified sampling (by month) → bootstrap distribution

**Task:** Build a **month-stratified** sample that takes the same number of observations per month.

This is one way to make your sample more representative when the population has strong seasonality.

```{r ex3_stratified_sample, echo=TRUE, eval=FALSE}
set.seed(480)

n_samp <- 240  # divisible by 12 makes this easy (20 per month)
k_per_month <- n_samp / 12

# 1) Create a month-stratified sample from df
df_strat <- df %>%
  filter(!is.na(swc)) %>%
  group_by(month) %>%
  slice_sample(n = k_per_month) %>%
  ungroup()

swc_samp_strat <- df_strat$swc

# CHECK:
df_strat %>% count(month)
```

```{r ex3_strat_bootstrap, echo=TRUE, eval=FALSE}
set.seed(480)

B <- 4000

boot_means_strat <- replicate(
  B,
  mean(sample(swc_samp_strat, size = length(swc_samp_strat), replace = TRUE))
)

tibble(
  pop_mean = pop_mean_swc,
  sample_mean = mean(swc_samp_strat),
  boot_sd = sd(boot_means_strat)
)
```

```{r ex3_plot_strat_boot, echo=TRUE, eval=FALSE}
# GOAL: Plot stratified bootstrap and compare to population mean.

tibble(boot_mean = boot_means_strat) %>%
  ggplot(aes(x = boot_mean)) +
  geom_histogram(bins = 50, alpha = 0.85) +
  geom_vline(xintercept = mean(swc_samp_strat), linetype = "dashed", linewidth = 1.1) +
  geom_vline(xintercept = pop_mean_swc, linetype = "solid", linewidth = 1.1) +
  theme_classic(base_size = 18) +
  labs(
    x = "Bootstrap mean (swc)",
    y = "Count",
    title = "Bootstrap distribution (month-stratified sample)",
    subtitle = "Dashed = sample mean; Solid = population mean"
  )
```

**Prompt (4–6 sentences):** Compare the centers and spreads of the three bootstrap distributions (random vs summer-only vs stratified).  
Which sampling design is most accurate for the annual mean? Which looks most precise?  
What does this teach you about bootstrap and study design?

> *Write your answer here.*

---

# Part II wrap-up

**One sentence to remember:**

> Bootstrap measures uncertainty around your estimate, **given your sample** — it cannot rescue a biased sample.


---

# Submission

- Knit your `.Rmd` to HTML.
- Commit and push both the `.Rmd` to your GitHub repo.
